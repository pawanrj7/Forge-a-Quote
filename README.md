# Image-Captioning
It is a machine Learning model named Forge-a-Quote, that generates natural language descriptions of
images and their regions. Our approach leverages datasets of images and their sentence
descriptions to learn about the inter-modal correspondences between language and visual
data. Our alignment model is based on a novel combination of Convolutional Neural
Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a
structured objective that aligns the two modalities through a multimodal embedding. We then
describe a Multimodal Recurrent Neural Network architecture that uses the inferred
alignments to learn to generate novel descriptions of image regions.

A solution requires both that the content of the image be understood and translated to
meaning in the terms of words, and that the words must string together to be comprehensible.
It combines both computer vision and natural language processing and marks a true
challenging problem in broader artificial intelligence. Automatically describing the content of 
an image is a fundamental problem in artificial intelligence that connects computer vision and
natural language processing.

##Here are some Screenshots---

![Screenshot1](https://github.com/pawanrj7/Forge-a-Quote/blob/master/Screenshots/Screenshot1.jpeg)

![Screenshot2](https://github.com/pawanrj7/Forge-a-Quote/blob/master/Screenshots/Screenshot2.jpeg)

![Screenshot3](https://github.com/pawanrj7/Forge-a-Quote/blob/master/Screenshots/Screenshot3.jpeg)


